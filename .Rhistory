model <- train(id ~ ., data = laptop_data_imputed, method = "lm", trControl = ctrl)
set.seed(1)
# Perform k-means clustering on a subset of numerical features
kmeans_imbalanced <- kmeans(numerical_columns[,1:8],2,nstart = 20)
# Analyze cluster distribution
# This table will show the distribution of "Priciness" levels within each cluster
table(kmeans_imbalanced$cluster,numerical_columns$Priciness)
set.seed(1)
# Perform k-means clustering on a subset of numerical features
kmeans_imbalanced <- kmeans(numerical_columns[,1:8],2,nstart = 20)
# Analyze cluster distribution
# This table will show the distribution of "Priciness" levels within each cluster
table(kmeans_imbalanced$cluster,numerical_columns$Priciness)
# Create a new data frame with the selected categorical columns
categorical_columns <- data.frame(laptop_data[,c(1,2,7,11,17,18,19)])
# Convert the "Priciness" column to a factor variable
categorical_columns$Priciness <- as.factor(categorical_columns$Priciness)
set.seed(1)
# Perform K-Modes clustering with 2 clusters
kmodes_imbalanced <- kmodes(categorical_columns[,1:6], 2, iter.max = 10, weighted = FALSE)
# Create a cross-tabulation table of cluster vs. price category
table(kmodes_imbalanced$cluster,categorical_columns$Priciness)
categoric_data_under <- ovun.sample(Priciness ~ ., data = categorical_columns, method = "under", N = 1072, seed = 1)$data
set.seed(1)
kmodes_balanced_c <- kmodes(categoric_data_under[,1:6], 2, iter.max = 10, weighted = FALSE)
table(kmodes_balanced_c$cluster,categoric_data_under$Priciness)
train_imbalance$Priciness <- as.factor(train_imbalance$Priciness)
test_imbalance$Priciness <- as.factor(test_imbalance$Priciness)
imbalanced_rf_model <- randomForest(Priciness~., data=train_imbalance, proximity=TRUE)
imbalanced_rf_model
predicted_rf <- predict(imbalanced_rf_model, test_imbalance)
confusionMatrix(predicted_rf, test_imbalance$Priciness)
train_balance$Priciness <- as.factor(train_balance$Priciness)
test_balance$Priciness <- as.factor(test_balance$Priciness)
balanced_rf_model <- randomForest(Priciness~., data=train_balance, proximity=TRUE)
balanced_rf_model
predicted_rf <- predict(balanced_rf_model, test_balance)
confusionMatrix(predicted_rf, test_balance$Priciness)
laptop_data_NA <- laptop_data
# Specify observations to delete
remove_rate <- round(nrow(laptop_data_NA) *0.20)
remove_data <- sample(1:nrow(laptop_data_NA), remove_rate)
# Replace specified observations with NA
laptop_data_NA[remove_data, "Price"] <- NA
laptop_data_imputed <- laptop_data
# Specify observations to delete
remove_rate <- round(nrow(laptop_data_imputed) *0.20)
remove_data <- sample(1:nrow(laptop_data_imputed), remove_rate)
# Replace specified observations with NA
laptop_data_imputed[remove_data, "Price"] <- NA
# Check data after processing
head(laptop_data_imputed)
# Analyze missing values
imputed <- skim(laptop_data_imputed)
imputed[,1:19]
laptop_data_imputed <- predict(missingdata_model, newdata = laptop_data_imputed)
# Impute missing values
missingdata_model <- preProcess(laptop_data_imputed, method='knnImpute')
missingdata_model
laptop_data_imputed <- predict(missingdata_model, newdata = laptop_data_imputed)
anyNA(laptop_data_imputed)
# Take average of categorical column
missing_mean_price <- mean(laptop_data_imputed$Price)
# Identify values that are higher or lower than average
laptop_data_imputed$Priciness <- ifelse(laptop_data_imputed$Price < missing_mean_price, "0", "1")
# Set random seed for reproducibility
set.seed(123)
# Convert Priciness to factor
laptop_data_imputed$Priciness <- as.factor(laptop_data_imputed$Priciness)
# Create imbalanced stratified sample
sample_imputed <- sample(2,nrow(laptop_data_imputed),replace=T,prob = c(0.8,0.2))
# Split data into training and test sets
train_imputed<-laptop_data_imputed[sample_imputed==1,]
test_imputed<-laptop_data_imputed[sample_imputed==2,]
# Build random forest model with proximity
missing_rf_model <- randomForest(Priciness~., data=train_imputed, proximity=TRUE)
missing_rf_model
# Predict Priciness on test data
predicted_rf <- predict(missing_rf_model, test_imputed)
confusionMatrix(predicted_rf, test_imputed$Priciness)
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(id ~ ., data = imbalanced_laptop_data, method = "lm", trControl = ctrl)
# View the structure of the data frame
str(laptop_data)
# Get summary statistics for each numerical variable
summary(laptop_data)
laptop_data_with_ıd <- read.csv("C:/Users/monster/Desktop/laptop_data.csv")
laptop_data_with_ıd <- read.csv("C:/Users/monster/Desktop/laptop_data.csv")
head(laptop_data_with_ıd)
laptop_data_with_ıd <- read.csv("C:/Users/monster/Desktop/laptop_data.csv")
head(laptop_data_with_ıd)
imbalanced_laptop_data$X <-laptop_data_with_ıd$X
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(X~ ., data = imbalanced_laptop_data, method = "lm", trControl = ctrl)
print(model)
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(X~ ., data = imbalanced_laptop_data, method = "lm", trControl = ctrl)
print(model)
model$finalModel
#We can use the following code to view the model predictions made for each fold
model$resample
#specify the cross-validation method
ctrl2 <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model2 <- train(id ~ ., data = oversampled_stroke, method = "lm", trControl = ctrl)
# Create a new data frame with balanced data
balance_Id_under <- ovun.sample(Priciness ~ ., data = imbalanced_laptop_data, method = "under", N = 1072, seed = 1)$data
#specify the cross-validation method
ctrl2 <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model2 <- train(X ~ ., data = balance_Id_under, method = "lm", trControl = ctrl)
print(model2)
model2$finalModel
#We can use the following code to view the model predictions made for each fold
model2$resample
# Create a new data frame with balanced data
balance_Id_under <- ovun.sample(Priciness ~ ., data = imbalanced_laptop_data, method = "under", N = 1072, seed = 1)$data
#specify the cross-validation method
ctrl2 <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model2 <- train(X ~ ., data = balance_Id_under, method = "lm", trControl = ctrl)
print(model2)
model2$finalModel
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(X~ ., data = imbalanced_laptop_data, method = "lm", trControl = ctrl)
print(model)
laptop_data_with_ıd <- read.csv("C:/Users/monster/Desktop/laptop_data.csv")
head(laptop_data_with_ıd)
numerical_columns$X <-laptop_data_with_ıd$X
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(X~ ., data = numerical_columns, method = "lm", trControl = ctrl)
print(model)
# Create a new data frame with balanced data
balance_Id_numeric_under <- ovun.sample(Priciness ~ ., data = numerical_columns, method = "under", N = 1072, seed = 1)$data
#specify the cross-validation method
ctrl2 <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model2 <- train(X ~ ., data = balance_Id_numeric_under, method = "lm", trControl = ctrl)
print(model2)
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ "", laptop_data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
}
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ "", data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
}
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
}
# En iyi performansı veren modeli seç
best_model <- results[which.min(sapply(results, function(x) x$rsq))]
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
}
# rsq değerlerini bir vektöre dönüştürün
rsq_values <- sapply(results, function(x) x$rsq)
# En iyi performansı veren modeli seçin
best_model_index <- which.min(rsq_values)
print(rsq_values)
print(results)
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
predictions[[i]] <-predict(model, test_imbalance)
}
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
predictions[[i]] <-predict(model, test_imbalance)
}
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
predictions <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
predictions[[i]] <-predict(model, test_imbalance)
}
print(predictions)
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
predictions <- list()
matricies <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
predictions[[i]] <-predict(model, test_imbalance)
predicted_rf <- predict(model, test_imbalance)
matricies <- confusionMatrix(predicted_rf, test_imbalance$Priciness)
}
print(matricies)
library(randomForest)
# Hedef değişkeni tanımla
y <- train_imbalance$Priciness
# Hiperparametre aralığını belirle
hyper_parameters <- expand.grid(
n_estimators = c(100, 200),
max_depth = c(5, 10),
min_samples_split = c(5, 10),
min_samples_leaf = c(5, 10),
mtry = c(2, 3)
)
# Grid aramayı gerçekleştir
results <- list()
predictions <- list()
matricies <- list()
for (i in 1:nrow(hyper_parameters)) {
model <- randomForest(y ~ ., data = train_imbalance, ntree = hyper_parameters$n_estimators[i], maxdepth = hyper_parameters$max_depth[i], min_samples_split = hyper_parameters$min_samples_split[i], min_samples_leaf = hyper_parameters$min_samples_leaf[i], mtry = hyper_parameters$mtry[i])
results[[i]] <- model
predictions[[i]] <-predict(model, test_imbalance)
predicted_rf <- predict(model, test_imbalance)
matricies[[i]] <- confusionMatrix(predicted_rf, test_imbalance$Priciness)
}
print(matricies)
# Dengeli veri kümesi için KNN sınıflandırması
library(kknn)
# KNN sınıflandırıcısını oluştur
knn_imbalance <- kknn(train_imbalance[, c("Priciness")], test_imbalance[, c("Priciness")], k = 5)
library(class)
knn_pred <- knn(
train = train_imbalance,
test = test_imbalance,
cl = train$Priciness,
k=10
)
library(class)
knn_pred <- knn(
train = train_imbalance,
test = test_imbalance,
cl = train_imbalance$Priciness,
k=10
)
library(class)
knn_pred <- knn(
train = train_imbalance,
test = test_imbalance,
cl = train$Priciness,
k=10
)
library(class)
knn_pred <- knn(
train = train_imbalance,
test = test_imbalance,
cl = train$Priciness,
k=10
)
library(class)
knn(
train = train_imbalance,
test = test_imbalance,
cl = train$Priciness,
k=10
)
library(class)
cl <- train$Priciness
library(class)
cl <- train$Priciness
cl <- as.data.frame(train$Priciness)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
library(naivebayes)
library(dplyr)
library(ggplot2)
install.packages("psych")
library(psych)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T)
model plot(model)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T)
print(model)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T)
pred <- predict(model, test_imbalance)
confusion_matrix <- table(pred, test_imbalance$Priciness)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T)
print(model)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T, , laplace = 1)
print(model)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T, , laplace = 1)
pred <- predict(model, test_imbalance)
confusion_matrix <- table(pred, test_imbalance$Priciness)
library(naivebayes)
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T, , laplace = 1)
print(model)
head(laptop_data)
laptop_data_imputed <- laptop_data
# Specify observations to delete
remove_rate <- round(nrow(laptop_data_imputed) *0.10)
remove_data <- sample(1:nrow(laptop_data_imputed), remove_rate)
# Replace specified observations with NA
laptop_data_imputed[remove_data, "Cpu_GHz"] <- NA
laptop_data_imputed[remove_data, "Ram"] <- NA
laptop_data_NA <- laptop_data
# Specify observations to delete
remove_rate <- round(nrow(laptop_data_NA) *0.10)
remove_data <- sample(1:nrow(laptop_data_NA), remove_rate)
# Replace specified observations with NA
laptop_data_NA[remove_data, "Cpu_GHz"] <- NA
laptop_data_NA[remove_data, "Ram"] <- NA
laptop_data_imputed <- laptop_data
# Specify observations to delete
remove_rate <- round(nrow(laptop_data_imputed) *0.10)
remove_data <- sample(1:nrow(laptop_data_imputed), remove_rate)
# Replace specified observations with NA
laptop_data_imputed[remove_data, "Cpu_GHz"] <- NA
laptop_data_imputed[remove_data, "Ram"] <- NA
# Analyze missing values
imputed <- skim(laptop_data_imputed)
imputed[,1:19]
# Impute missing values
missingdata_model <- preProcess(laptop_data_imputed, method='knnImpute')
missingdata_model
laptop_data_imputed <- predict(missingdata_model, newdata = laptop_data_imputed)
anyNA(laptop_data_imputed)
# Take average of categorical column
missing_mean_price <- mean(laptop_data_imputed$Price)
# Identify values that are higher or lower than average
laptop_data_imputed$Priciness <- ifelse(laptop_data_imputed$Price < missing_mean_price, "0", "1")
# Set random seed for reproducibility
set.seed(123)
# Convert Priciness to factor
laptop_data_imputed$Priciness <- as.factor(laptop_data_imputed$Priciness)
# Create imbalanced stratified sample
sample_imputed <- sample(2,nrow(laptop_data_imputed),replace=T,prob = c(0.8,0.2))
# Split data into training and test sets
train_imputed<-laptop_data_imputed[sample_imputed==1,]
test_imputed<-laptop_data_imputed[sample_imputed==2,]
# Build random forest model with proximity
missing_rf_model <- randomForest(Priciness~., data=train_imputed, proximity=TRUE)
missing_rf_model
# Predict Priciness on test data
predicted_rf <- predict(missing_rf_model, test_imputed)
confusionMatrix(predicted_rf, test_imputed$Priciness)
varImpPlot(missing_rf_model)
# Set random seed for reproducibility
set.seed(123)
# Convert Priciness to factor
laptop_data_NA$Priciness <- as.factor(laptop_data_NA$Priciness)
# Create imbalanced stratified sample
sample_NA <- sample(2,nrow(laptop_data_NA),replace=T,prob = c(0.8,0.2))
# Split data into training and test sets
train_NA<-laptop_data_NA[sample_NA==1,]
test_NA<-laptop_data_NA[sample_NA==2,]
# Build random forest model with proximity
NA_rf_model <- randomForest(Priciness~., data=train_NA, proximity=TRUE)
train_drop_NA <- drop_na(train_NA)
# Build random forest model with proximity
NA_rf_model <- randomForest(Priciness~., data=train_drop_NA, proximity=TRUE)
NA_rf_model
test_drop_NA <- drop_na(test_NA)
# Predict Priciness on test data
predicted_rf <- predict(NA_rf_model, test_drop_NA)
confusionMatrix(predicted_rf, test_drop_NA$Priciness)
varImpPlot(NA_rf_model)
library(naivebayes)
# Train the Naive Bayes model using the "Priciness" column as the classification variable.
# The "usekernel = T" and "laplace = 1" parameters enable the model to be created using a Gaussian distribution and Laplace correction.
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T, laplace = 1)
# Print the structure of the model.
print(model)
library(naivebayes)
# Train the Naive Bayes model using the "Priciness" column as the classification variable.
# The "usekernel = T" and "laplace = 1" parameters enable the model to be created using a Gaussian distribution and Laplace correction.
model <- naive_bayes(Priciness ~ ., data = train_imbalance, usekernel = T, laplace = 1)
predicted_values <- predict(model, newdata = test_imbalance)
library(caret)
confusionMatrix(predicted_values, test_imbalance$Priciness)
# Make predictions on the test set.
predicted_rf <- predict(imbalanced_rf_model, test_imbalance)
# Evaluate the accuracy and error rates of the predictions.
confusionMatrix(predicted_rf, test_imbalance$Priciness)
train_drop_NA <- drop_na(train_NA)
# Build random forest model with proximity
NA_rf_model <- randomForest(Priciness~., data=train_drop_NA, proximity=TRUE)
NA_rf_model
test_drop_NA <- drop_na(test_NA)
# Predict Priciness on test data
predicted_rf <- predict(NA_rf_model, test_drop_NA)
confusionMatrix(predicted_rf, test_drop_NA$Priciness)
print(matricies)
#specify the cross-validation method
ctrl <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model <- train(X~ ., data = numerical_columns, method = "lm", trControl = ctrl)
print(model)
#specify the cross-validation method
ctrl2 <- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model2 <- train(X ~ ., data = balance_Id_numeric_under, method = "lm", trControl = ctrl)
print(model2)
